---
title: "Imputing Missing Values: Random Forest vs K-Nearest Neighbors"
author: Robert Adongo  
date: "June 7, 2025"
output:
  # cleanrmd::html_document_clean:
  #   theme: NULL
  #   toc: true
  #   toc_float: true
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    number_sections: true
    theme: flatly
    highlight: tango
    df_print: paged
    #css: "custom.css"
bibliography: references.bib
csl: apa.csl
link-citations: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
## Motivation and Goals 
Missing data poses a significant barrier to effective analysis in many applied research domains. The K-Nearest Neighbors (KNN) method is a simple and widely used imputation technique [@troyanskaya2001missing]. In this project, we implement the `kNN` function from the `VIM` package, which is based on a variation of the Gower distance. This approach is suitable for handling numerical, categorical, ordered, and semi-continuous variables [@kowarik2016imputation].

We also utilize Random Forest (RF), a non-parametric ensemble method, for imputation. Specifically, we employ the `missForest` function from the `missForestPredict` package [@troyanskaya2001missing; @van2011mice; @stekhoven2012missforest; @wright2017ranger].

The goal of this document is to provide a theoretical background for the two imputation methods mentioned above, apply them to datasets with different structures and missingness patterns, and compare their performance.

## The `KNN` Function
`kNN(
  data,
  variable = colnames(data),
  metric = NULL,
  k = 5,
  dist_var = colnames(data),
  weights = NULL,
  numFun = median,
  catFun = maxCat,
  makeNA = NULL,
  NAcond = NULL,
  impNA = TRUE,
  donorcond = NULL,
  mixed = vector(),
  mixed.constant = NULL,
  trace = FALSE,
  imp_var = TRUE,
  imp_suffix = "imp",
  addRF = FALSE,
  onlyRF = FALSE,
  addRandom = FALSE,
  useImputedDist = TRUE,
  weightDist = FALSE,
  methodStand = "range",
  ordFun = medianSamp
)`

## The `missForest` Function
`missForest(
  xmis,
  maxiter = 10,
  fixed_maxiter = FALSE,
  var_weights = NULL,
  decreasing = FALSE,
  initialization = "mean/mode",
  x_init = NULL,
  class.weights = NULL,
  return_integer_as_integer = FALSE,
  save_models = TRUE,
  predictor_matrix = NULL,
  proportion_usable_cases = c(1, 0),
  verbose = TRUE,
  convergence_error = "OOB",
  ...
)`

# Mathematical Setup
## K Nearest Neighbor Imputation

### Model Form
The mathematical setup presented in this section is based on the work by Kowarik and Templ [@kowarik2016imputation], who provide a comprehensive formulation of k-nearest neighbor imputation as implemented in the `VIM` package.

Suppose we observe a data set with missing values. Let \( \mathbf{x}_i = (x_{i1}, x_{i2}, \dots, x_{ip}) \) represent the observed values for observation \( i \) across \( p \) variables, where some entries in \( \mathbf{x}_i \) may be missing.

The goal of k-nearest neighbor (kNN) imputation is to fill in the missing entries of \( \mathbf{x}_i \) using information from the **k most similar (complete) observations**.

Similarity between two observations \( \mathbf{x}_i \) and \( \mathbf{x}_j \) is measured by a **generalized Gower distance**, which is defined to accommodate numerical, categorical, ordinal, binary, and semi-continuous variables [@gower1971general].

### Generalized Gower Distance

Let \( d_{ij} \) denote the distance between observations \( i \) and \( j \). Then:

\[
d_{ij} = \frac{\sum_{k=1}^p w_k \delta_{ijk}}{\sum_{k=1}^p w_k}
\]

where:
 \( w_k \geq 0 \) is the weight assigned to variable \( k \),
 \( \delta_{ijk} \in [0, 1] \) measures the contribution of variable \( k \) to the overall distance between \( i \) and \( j \).

The form of \( \delta_{ijk} \) depends on the type of variable \( k \):

- **Continuous variables**:

\[
\delta_{ijk} = \frac{|x_{ik} - x_{jk}|}{r_k}, \quad \text{where } r_k = \max(x_{\cdot k}) - \min(x_{\cdot k})
\]

- **Ordinal variables** (converted to integers):

\[
\delta_{ijk} = \frac{|x_{ik} - x_{jk}|}{r_k}
\]

- **Nominal or binary variables**:

\[
\delta_{ijk} =
\begin{cases}
0, & \text{if } x_{ik} = x_{jk} \\
1, & \text{otherwise}
\end{cases}
\]

- **Semi-continuous variables** (e.g., income with many zeros):

\[
\delta_{ijk} =
\begin{cases}
0, & \text{if } x_{ik} = s_k \text{ and } x_{jk} = s_k \\
1, & \text{if exactly one of } x_{ik}, x_{jk} \text{ equals } s_k \\
\frac{|x_{ik} - x_{jk}|}{r_k}, & \text{if } x_{ik} \neq s_k \text{ and } x_{jk} \neq s_k
\end{cases}
\]

Here, \( s_k \) is the “special” value (typically zero) where the variable has mass.

### Aggregation Rule

Once the distances \( d_{ij} \) are computed, the **k nearest neighbors** of observation \( i \) (with complete data) are identified. The missing entries of \( \mathbf{x}_i \) are then imputed using an aggregation of the corresponding values from its neighbors:

- For **continuous variables**:

\[
\hat{x}_{ik} = \text{median}\left\{x_{jk} : j \in \mathcal{N}_k(i) \right\}
\]

- For **categorical variables**:

\[
\hat{x}_{ik} = \text{mode}\left\{x_{jk} : j \in \mathcal{N}_k(i) \right\}
\]

where \( \mathcal{N}_k(i) \) denotes the set of indices of the k nearest neighbors for observation \( i \) (based on distance in selected variables).

### Tuning Parameter \( k \)

The parameter \( k \in \mathbb{N} \) controls the number of nearest neighbors used in imputation. Small values of \( k \) may lead to high variance; large values may oversmooth or dilute local structure.

## Random Forest Imputation

The mathematical setup presented in this section is based on the work of Stekhoven and Bühlmann [@stekhoven2012missforest], who introduced the missForest algorithm for non-parametric imputation of mixed-type data using Random Forests.

### Data Structure and Objective

Let \( \mathbf{X} = (x_{ij}) \in \mathbb{R}^{n \times p} \) be a data matrix with \( n \) observations and \( p \) variables. Denote by \( \mathcal{M}_j \subseteq \{1, \dots, n\} \) the set of indices with missing values in variable \( j \), and let \( \mathcal{O}_j = \{1, \dots, n\} \setminus \mathcal{M}_j \) be the observed entries for that variable.

The goal is to impute all missing values in \( \mathbf{X} \) using a non-parametric model without making distributional assumptions.

### Iterative Imputation Scheme

The missForest algorithm proceeds iteratively. Let \( \mathbf{X}^{(0)} \) be the initial matrix with missing values imputed using mean imputation (numerical) or mode imputation (categorical). For iteration \( t = 1, 2, \dots \), the procedure is:

1. Define a variable-wise ordering such that variables with fewer missing values are imputed first.
2. For each variable \( j \in \{1, \dots, p\} \):
   - Let \( \mathbf{Y}_{\mathcal{O}_j} = \{x_{ij} : i \in \mathcal{O}_j\} \) be the observed responses.
   - Let \( \mathbf{X}_{\mathcal{O}_j, -j}^{(t-1)} \) be the corresponding predictors (excluding \( j \)) for observed rows.
   - Train a Random Forest regression or classification model \( f_j^{(t)} \) using:
     \[
     f_j^{(t)} : \mathbf{X}_{\mathcal{O}_j, -j}^{(t-1)} \rightarrow \mathbf{Y}_{\mathcal{O}_j}
     \]
   - Predict missing values \( \hat{x}_{ij}^{(t)} = f_j^{(t)}(\mathbf{X}_{i, -j}^{(t-1)}) \) for \( i \in \mathcal{M}_j \).
3. Repeat the process for all variables \( j = 1, \dots, p \).

The updated matrix \( \mathbf{X}^{(t)} \) combines observed values with new predictions for all variables.

### Convergence and Stopping

The iterations continue until a stopping criterion is met, such as:
- No substantial change in imputed values between \( \mathbf{X}^{(t)} \) and \( \mathbf{X}^{(t-1)} \),
- Or a fixed maximum number of iterations.

The normalized root mean squared error (NRMSE) or proportion of falsely classified entries (PFC) can be used to assess convergence and performance.


# Example 1; Continuous Variables only
## Over
# References