---
title: "Imputing Missing Values: Random Forest vs K-Nearest Neighbors"
author: Robert Adongo  
date: "June 7, 2025"
output:
  # cleanrmd::html_document_clean:
  #   theme: NULL
  #   toc: true
  #   toc_float: true
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    number_sections: true
    theme: flatly
    highlight: tango
    df_print: paged
    #css: "custom.css"
bibliography: references.bib
csl: apa.csl
link-citations: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
## Motivation and Goals 
Missing data poses a significant barrier to effective analysis in many applied research domains. The K-Nearest Neighbors (KNN) method is a simple and widely used imputation technique [@troyanskaya2001missing]. In this project, we implement the `kNN` function from the `VIM` package, which is based on a variation of the Gower distance. This approach is suitable for handling numerical, categorical, ordered, and semi-continuous variables [@kowarik2016imputation].

We also utilize Random Forest (RF), a non-parametric ensemble method, for imputation. Specifically, we employ the `missForest` function from the `missForestPredict` package [@troyanskaya2001missing; @van2011mice; @stekhoven2012missforest; @wright2017ranger].

The goal of this document is to provide a theoretical background for the two imputation methods mentioned above, apply them to datasets with different structures and missingness patterns, and compare their performance.

## The `KNN` Function
`kNN(
  data,
  variable = colnames(data),
  metric = NULL,
  k = 5,
  dist_var = colnames(data),
  weights = NULL,
  numFun = median,
  catFun = maxCat,
  makeNA = NULL,
  NAcond = NULL,
  impNA = TRUE,
  donorcond = NULL,
  mixed = vector(),
  mixed.constant = NULL,
  trace = FALSE,
  imp_var = TRUE,
  imp_suffix = "imp",
  addRF = FALSE,
  onlyRF = FALSE,
  addRandom = FALSE,
  useImputedDist = TRUE,
  weightDist = FALSE,
  methodStand = "range",
  ordFun = medianSamp
)`

## The `missForest` Function
`missForest(
  xmis,
  maxiter = 10,
  fixed_maxiter = FALSE,
  var_weights = NULL,
  decreasing = FALSE,
  initialization = "mean/mode",
  x_init = NULL,
  class.weights = NULL,
  return_integer_as_integer = FALSE,
  save_models = TRUE,
  predictor_matrix = NULL,
  proportion_usable_cases = c(1, 0),
  verbose = TRUE,
  convergence_error = "OOB",
  ...
)`

# Mathematical Setup
## K Nearest Neighbor Imputation

### Model Form
The mathematical setup presented in this section is based on the work by Kowarik and Templ [@kowarik2016imputation], who provide a comprehensive formulation of k-nearest neighbor imputation as implemented in the `VIM` package.

Suppose we observe a data set with missing values. Let \( \mathbf{x}_i = (x_{i1}, x_{i2}, \dots, x_{ip}) \) represent the observed values for observation \( i \) across \( p \) variables, where some entries in \( \mathbf{x}_i \) may be missing.

The goal of k-nearest neighbor (kNN) imputation is to fill in the missing entries of \( \mathbf{x}_i \) using information from the **k most similar (complete) observations**.

Similarity between two observations \( \mathbf{x}_i \) and \( \mathbf{x}_j \) is measured by a **generalized Gower distance**, which is defined to accommodate numerical, categorical, ordinal, binary, and semi-continuous variables [@gower1971general].

### Generalized Gower Distance

Let \( d_{ij} \) denote the distance between observations \( i \) and \( j \). Then:

\[
d_{ij} = \frac{\sum_{k=1}^p w_k \delta_{ijk}}{\sum_{k=1}^p w_k}
\]

where:
 \( w_k \geq 0 \) is the weight assigned to variable \( k \),
 \( \delta_{ijk} \in [0, 1] \) measures the contribution of variable \( k \) to the overall distance between \( i \) and \( j \).

The form of \( \delta_{ijk} \) depends on the type of variable \( k \):

- **Continuous variables**:

\[
\delta_{ijk} = \frac{|x_{ik} - x_{jk}|}{r_k}, \quad \text{where } r_k = \max(x_{\cdot k}) - \min(x_{\cdot k})
\]

- **Ordinal variables** (converted to integers):

\[
\delta_{ijk} = \frac{|x_{ik} - x_{jk}|}{r_k}
\]

- **Nominal or binary variables**:

\[
\delta_{ijk} =
\begin{cases}
0, & \text{if } x_{ik} = x_{jk} \\
1, & \text{otherwise}
\end{cases}
\]

- **Semi-continuous variables** (e.g., income with many zeros):

\[
\delta_{ijk} =
\begin{cases}
0, & \text{if } x_{ik} = s_k \text{ and } x_{jk} = s_k \\
1, & \text{if exactly one of } x_{ik}, x_{jk} \text{ equals } s_k \\
\frac{|x_{ik} - x_{jk}|}{r_k}, & \text{if } x_{ik} \neq s_k \text{ and } x_{jk} \neq s_k
\end{cases}
\]

Here, \( s_k \) is the “special” value (typically zero) where the variable has mass.

### Aggregation Rule

Once the distances \( d_{ij} \) are computed, the **k nearest neighbors** of observation \( i \) (with complete data) are identified. The missing entries of \( \mathbf{x}_i \) are then imputed using an aggregation of the corresponding values from its neighbors:

- For **continuous variables**:

\[
\hat{x}_{ik} = \text{median}\left\{x_{jk} : j \in \mathcal{N}_k(i) \right\}
\]

- For **categorical variables**:

\[
\hat{x}_{ik} = \text{mode}\left\{x_{jk} : j \in \mathcal{N}_k(i) \right\}
\]

where \( \mathcal{N}_k(i) \) denotes the set of indices of the k nearest neighbors for observation \( i \) (based on distance in selected variables).

### Tuning Parameter \( k \)

The parameter \( k \in \mathbb{N} \) controls the number of nearest neighbors used in imputation. Small values of \( k \) may lead to high variance; large values may oversmooth or dilute local structure.

## Random Forest Imputation

The Random Forest-based imputation approach used in this study is based on the missForest algorithm introduced by Stekhoven and Bühlmann [@stekhoven2012missforest]. This method uses iterative, variable-wise Random Forest models to estimate missing entries in a mixed-type data matrix without relying on parametric assumptions.

---

### Data Structure and Objective

Let \( \mathbf{X} = (x_{ij}) \in \mathbb{R}^{n \times p} \) denote a data matrix with \( n \) observations and \( p \) variables. Some entries in \( \mathbf{X} \) are missing.

For each variable \( j \in \{1, \dots, p\} \), define
\( \mathcal{M}_j = \{ i \in \{1, \dots, n\} : x_{ij} \text{ is missing} \} \) — the indices with missing entries,
and \( \mathcal{O}_j = \{1, \dots, n\} \setminus \mathcal{M}_j \) — the indices with observed entries.

The objective is to impute all missing values in \( \mathbf{X} \) using an iterative, non-parametric regression/classification scheme based on Random Forests.

---

### Initialization

Let \( \mathbf{X}^{(0)} \) be the initial imputed matrix, constructed by applying:
- Mean imputation for continuous variables,
- Mode imputation for categorical variables.

This serves as the starting point for the iterative procedure.

---

### Iterative Imputation Scheme

For each iteration \( t = 1, 2, \dots \), the following steps are performed:

1. Determine a variable ordering from least to most missing values.
2. For each variable \( j \in \{1, \dots, p\} \):
   - Define:
     \[
     \mathbf{Y}_j = \left( x_{ij} \right)_{i \in \mathcal{O}_j}, \quad
     \mathbf{Z}_j = \left( \mathbf{x}_{i, -j}^{(t-1)} \right)_{i \in \mathcal{O}_j}
     \]
   - Train a Random Forest model \( \hat{f}_j^{(t)} \) using observed data \( (\mathbf{Z}_j, \mathbf{Y}_j) \).
   - Predict missing entries for \( i \in \mathcal{M}_j \) via:
     \[
     \hat{x}_{ij}^{(t)} = \hat{f}_j^{(t)} \left( \mathbf{x}_{i, -j}^{(t-1)} \right)
     \]
3. Replace the missing entries in variable \( j \) with \( \hat{x}_{ij}^{(t)} \) and proceed to the next variable.

After all variables have been processed, the updated matrix \( \mathbf{X}^{(t)} \) is used in the next iteration.

---

### Stopping Criterion

To determine convergence, the algorithm computes the change in imputed values between iterations \( t \) and \( t-1 \), separately for continuous and categorical variables.

- For continuous variables \( \mathcal{N} \):

\[
\Delta_{\mathcal{N}}^{(t)} = \frac{ \sum_{j \in \mathcal{N}} \left\| \mathbf{X}_j^{(t)} - \mathbf{X}_j^{(t-1)} \right\|^2 }
{ \sum_{j \in \mathcal{N}} \left\| \mathbf{X}_j^{(t)} \right\|^2 }
\]

- For categorical variables \( \mathcal{F} \):

\[
\Delta_{\mathcal{F}}^{(t)} = \frac{ \sum_{j \in \mathcal{F}} \sum_{i \in \mathcal{M}_j} \mathbb{I} \left( x_{ij}^{(t)} \ne x_{ij}^{(t-1)} \right) }{ \sum_{j \in \mathcal{F}} |\mathcal{M}_j| }
\]

The algorithm stops when either \( \Delta_{\mathcal{N}}^{(t)} \) or \( \Delta_{\mathcal{F}}^{(t)} \) increases compared to the previous iteration.

---

### Evaluation Metric

To assess imputation performance, the **Normalized Root Mean Squared Error (NRMSE)**[@oba2003bayesian] is used for continuous variables:

\[
\text{NRMSE} = \sqrt{ \frac{ \text{mean} \left( \left( \mathbf{X}_{\text{true}} - \mathbf{X}_{\text{imp}} \right)^2 \right) }{ \text{var} \left( \mathbf{X}_{\text{true}} \right) } }
\]

This is computed only over entries that were originally missing. A lower NRMSE indicates better imputation accuracy, and it provides a scale-invariant comparison between methods and datasets.

# Applications 
## Continuous Variables only
# References